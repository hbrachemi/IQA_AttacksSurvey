# IQA_AttacksSurvey

## Abstract
The rapid development of Deep Learning (DL) and,
more specifically, Convolutional Neural Networks (CNNs) has
achieved high accuracy over the past decade, becoming the
standard approach in computer vision in a short time. However,
recent studies have discovered that CNNs are vulnerable to
adversarial attacks in image classification tasks. While most
studies have focused on DL models for image classification, only
a few works have addressed this issue in the context of Image
Quality Assessment (IQA). This paper investigates the robustness
of different CNN models against adversarial attacks when used
for an IQA task. We propose an adaptation of state-of-the-
art image classification attacks in both targeted and untargeted
modes for an IQA regression task. We also analyze the correlation
between the perturbation’s visibility and the attack’s success.
Our experimental results show that DL-based IQA methods
are vulnerable to such attacks, with a significant decrease in
correlation scores when subjected to adversarial perturbations.
Consequently, the development of countermeasures against such
attacks is essential for improving the reliability and accuracy of
DL-based IQA models.
## General Attack Outline
![](https://github.com/hbrachemi/IQA_AttacksSurvey/blob/master/schema.png)
