{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7538b6",
   "metadata": {
    "id": "9c7538b6"
   },
   "outputs": [],
   "source": [
    "from architectures import *\n",
    "from dataloader import iqa_dataset\n",
    "from config import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from weightingMaps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caca89",
   "metadata": {
    "id": "85caca89"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.ndimage import sobel\n",
    "from attacks.fgm import fast_gradient_method \n",
    "from attacks.pgd import projected_gradient_descent \n",
    "from eval_funct import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82524027",
   "metadata": {
    "id": "82524027"
   },
   "outputs": [],
   "source": [
    "db_path ='../Databases/tid2013/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff39a22",
   "metadata": {
    "id": "eff39a22"
   },
   "outputs": [],
   "source": [
    "scale = int(input()) #Scale of ground truth quality score used in the dataset\n",
    "\n",
    "if db_path.split(\"/\")[-2] == \"tid2013\":\n",
    "    scale2 = 10\n",
    "elif db_path.split(\"/\")[-2] == \"koniq\":\n",
    "    scale2 = 5\n",
    "elif db_path.split(\"/\")[-2] == \"livew\":\n",
    "    scale2 = 100\n",
    "else:\n",
    "    print(\"SCALE WASNT DEFINED\")\n",
    "if scale != scale2:\n",
    "        print(\"Check scale value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbcfe6",
   "metadata": {
    "id": "47dbcfe6",
    "outputId": "7ec10b99-2710-412e-bd37-d075df878aad"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_data=iqa_dataset(part='train',labels_path=db_path+'scores.pickle' ,db_path=db_path+'Images/',\n",
    "                 ids_path=db_path+'IDs.pickle',\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_data = iqa_dataset(part='test',labels_path=db_path+'scores.pickle' ,db_path=db_path+'Images/',\n",
    "                 ids_path=db_path+'IDs.pickle',\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_data,1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021913d",
   "metadata": {
    "id": "8021913d"
   },
   "outputs": [],
   "source": [
    "weights_path = '../pretrained/iqaModel_tid_resnet.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e608ebb",
   "metadata": {
    "id": "2e608ebb",
    "outputId": "1cb410b1-6e7f-4162-d60c-8953e2b09274"
   },
   "outputs": [],
   "source": [
    "resnet = RESNET(weights_path)\n",
    "resnet = resnet.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80eecf",
   "metadata": {
    "id": "cc80eecf"
   },
   "outputs": [],
   "source": [
    "SRCC,KRCC,PLCC,RMSE,fr,mos = evaluate(val_dl,scale,resnet.resnet,normalize_imagenet, None ,None)\n",
    "print(\"------------------- performance of the NR IQA metric: -------------------\")\n",
    "print(\"SRCC: \",SRCC)\n",
    "print(\"PLCC: \",PLCC)\n",
    "print(\"KRCC: \",KRCC)\n",
    "print(\"RMSE: \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14661a11",
   "metadata": {
    "id": "14661a11"
   },
   "outputs": [],
   "source": [
    "#Define parametters and attacks\n",
    "iterations = [10]\n",
    "epsilons = [0.1,1]\n",
    "attacks = [\"bim\"]\n",
    "losses = [\"mse(y_tielda,y)\",'sqr(max-y)']\n",
    "weights = [\"activationMaps\",\"gradients\"]\n",
    "use_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c91b44",
   "metadata": {
    "id": "48c91b44"
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "perf = dict()\n",
    "epsilon_dict = dict()\n",
    "iter_dict = dict()\n",
    "attack_dict = dict()\n",
    "loss_dict = dict()\n",
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b9913",
   "metadata": {
    "id": "6a7b9913"
   },
   "outputs": [],
   "source": [
    "perf['srcc'] = str(SRCC)\n",
    "perf['plcc'] = str(PLCC)\n",
    "perf['krcc'] = str(KRCC)\n",
    "perf['rmse'] = str(RMSE)\n",
    "results['original_performance'] = copy.deepcopy(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ffdbc",
   "metadata": {
    "id": "1b9ffdbc"
   },
   "outputs": [],
   "source": [
    "import pyiqa\n",
    "fr_metric = pyiqa.create_metric('lpips', device=device,as_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69dffd",
   "metadata": {
    "id": "db69dffd"
   },
   "outputs": [],
   "source": [
    "cnn_backbone = 'resnet'\n",
    "model = resnet.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a6543",
   "metadata": {
    "id": "0e3a6543"
   },
   "outputs": [],
   "source": [
    "w = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565f105",
   "metadata": {
    "id": "3565f105"
   },
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.MSELoss()\n",
    "y_target = torch.tensor(1).float().to(device)\n",
    "y_target = torch.unsqueeze(y_target,0)\n",
    "y_target = torch.unsqueeze(y_target,0)\n",
    "\n",
    "\n",
    "for loss in losses:\n",
    "    if loss in ['mse','mse(y_tielda,y)']:\n",
    "        targeted = False\n",
    "    else:\n",
    "        targeted = True\n",
    "    for attack in attacks:\n",
    "        fgm_passage = False\n",
    "        for it in iterations:\n",
    "            if attack == 'fgm' and fgm_passage:\n",
    "                continue\n",
    "            else:\n",
    "                fgm_passage = True\n",
    "\n",
    "\n",
    "            epsilon_dict = dict()\n",
    "\n",
    "\n",
    "            for epsilon in epsilons: \n",
    "                \n",
    "                y_adv_list = []\n",
    "                y = []\n",
    "                fr = []\n",
    "\n",
    "\n",
    "                for i ,[im, label] in enumerate(tqdm(val_dl)):\n",
    "                    im = im.to(device)\n",
    "\n",
    "                    if targeted == False:\n",
    "                      if loss == 'mse':\n",
    "                        #if mos are available\n",
    "                        y_target = torch.unsqueeze(label.float().to(device)/scale,0)\n",
    "                      if loss == 'mse(y_tielda,y)':\n",
    "                        #estimate mos\n",
    "                        y_pred = float(model(normalize_imagenet(im)).detach().cpu())*scale\n",
    "                        s = 0\n",
    "                        for counter in range(10):        \n",
    "                            s += np.random.normal(y_pred,3*float(results['original_performance']['rmse']),1)\n",
    "                        s /= 10\n",
    "                        s =torch.tensor(s)\n",
    "                        y_target = torch.unsqueeze(s.float().to(device)/scale,0)\n",
    "                        \n",
    "                    y.append(float(label.detach().cpu()))\n",
    "                    if attack == \"fgm\":\n",
    "                        img_adv = fast_gradient_method(model,im,epsilon,np.inf,\n",
    "                                                       preprocess=normalize_imagenet,y=y_target,\n",
    "                                                       loss_fn=loss_fct,targeted=targeted)\n",
    "                        \n",
    "                    if attack == \"pgd\":\n",
    "                        img_adv = projected_gradient_descent(model,im,epsilon,eps_iter=0.001,preprocess=normalize_imagenet,\n",
    "                                                             nb_iter=it,norm=np.inf,y=y_target,loss_fn=loss_fct,\n",
    "                                                             targeted=targeted,rand_init=True)\n",
    "                    if attack == \"bim\":\n",
    "                        x0 = torch.clone(im)\n",
    "                        for j in range(it):\n",
    "                            x0 = fast_gradient_method(model,x0,epsilon,np.inf,y=y_target,loss_fn=loss_fct,targeted=targeted,preprocess=normalize_imagenet)\n",
    "                        img_adv = x0\n",
    "                    \n",
    "                    if use_weights:\n",
    "                        if w == \"activationMaps\":\n",
    "                            pred = resnet(im)\n",
    "                            pred.backward()\n",
    "                            gradients = resnet.get_activations_gradient()\n",
    "                            # pool the gradients across the channels\n",
    "                            pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "                            # get the activations of the last convolutional layer\n",
    "                            activations = resnet.get_activations(im).detach()\n",
    "                            # weight the channels by corresponding gradients\n",
    "                            if cnn_backbone in [\"resnet\",\"inception\"]:\n",
    "                                n_c =2048\n",
    "                            if cnn_backbone ==\"vgg\":\n",
    "                                n_c = 512\n",
    "                            for k in range(n_c):\n",
    "                                activations[:, k, :, :] *= pooled_gradients[k]\n",
    "                                # average the channels of the activations\n",
    "                                heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "                                # relu on top of the heatmap\n",
    "                                # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "                                #heatmap = np.maximum(heatmap, 0)\n",
    "                                # normalize the heatmap\n",
    "                                heatmap /= torch.max(heatmap)\n",
    "                                heatmap = heatmap.unsqueeze(0)\n",
    "                                #convert to channeled image\n",
    "                                heatmap = torch.cat([heatmap,heatmap,heatmap], dim=0)\n",
    "                                #resize heatmap to fit image\n",
    "                                heatmap = torchvision.transforms.Resize((im.shape[-2],im.shape[-1]))(heatmap).unsqueeze(0)\n",
    "                                #retreive attack's noise signal and multiply by weight\n",
    "                                weighted_noise = (img_adv-im)*heatmap\n",
    "                                img_adv = im + weighted_noise\n",
    "                        if w == \"gradients\":\n",
    "                                #compute image's activity\n",
    "                                grad = torch.stack([torch.tensor(sobel(im[0,0,:,:])),torch.tensor(sobel(im[0,1,:,:])),torch.tensor(sobel(im[0,2,:,:]))])\n",
    "                                #retreive attack's noise signal and multiply by weight\n",
    "                                weighted_noise = (img_adv-im)*grad\n",
    "                                img_adv = im + weighted_noise\n",
    "                        \n",
    "                    y_adv_list.append(float(model(normalize_imagenet(img_adv)).detach().cpu()*scale))\n",
    "                    fr.append(fr_metric(im,img_adv).cpu().detach())\n",
    "                SRCC,KRCC,PLCC,RMSE = compute_metrics(y,y_adv_list)\n",
    "                \n",
    "\n",
    "                perf['srcc'] = str(SRCC)\n",
    "                perf['krcc'] = str(KRCC)\n",
    "                perf['plcc'] = str(PLCC)\n",
    "                perf['rmse'] = str(RMSE)\n",
    "                perf['lpips'] = str(np.mean(fr))\n",
    "                epsilon_dict[str(epsilon)] = copy.deepcopy(perf)\n",
    "                print(f'{loss}\\n {attack}\\n  {it}\\n   {epsilon_dict}')\n",
    "               \n",
    "            iter_dict[str(it)] = copy.deepcopy(epsilon_dict)\n",
    "        attack_dict[attack] = copy.deepcopy(iter_dict)\n",
    "    loss_dict[loss] = copy.deepcopy(attack_dict)\n",
    "    results['results'] = loss_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9bf51",
   "metadata": {
    "id": "29a9bf51"
   },
   "outputs": [],
   "source": [
    "f = 'file.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f0c30",
   "metadata": {
    "id": "e47f0c30"
   },
   "outputs": [],
   "source": [
    "with open(f, 'w') as outfile:\n",
    "    yaml.dump(results, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f28c",
   "metadata": {
    "id": "a319f28c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
